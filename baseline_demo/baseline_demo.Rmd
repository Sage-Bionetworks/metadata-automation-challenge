---
title: "Metadata Automation Challenge - Baseline Tool Demo"
author: James Eddy
date: January 3rd, 2020
output: 
  html_notebook: 
    highlight: pygments
    theme: yeti
---


# Summary

This demonstration serves two purposes: (1) illustrate elements and characteristics of input data, reference standards, and annotation results that are relevant to participants developing tools for the challenge; and (2) step through the process used by a simple "baseline" tool to produce results for an input dataset.

I acknowledge that there are a number of flaws and limitations in my approach below. My intent is to provide an orientation to how others might tackle challenge tasks. I expect that participants will be able to devise more creative and more sophisticated solutions that outperform my baseline tool!

---

# Setup

## Load packages and scoring functions

```{r, message=FALSE, warning=FALSE}
library(tidyverse)
library(fuzzyjoin)
library(synapser)
source("../R/baseline_annotator.R")
source("../R/scoring.R")
```


## Specify file locations

I've chosen to use folders at the root of my system path to store various inputs and outputs for the demo — as this matches the expected setup for the Docker image I'll eventually need to create. However, you might need special privilages to create these folders on your local machine.

You can change these paths as needed to point to other locations (including the current working directory).

```{r}
input_dir <- "/input/"
output_dir <- "/output/"
data_dir <- "/data/"
user_data_dir <- "./"
```


## Load reference data

The **caDSR export** table can be accessed and downloaded from Synapse [**here**](https://www.synapse.org/#!Synapse:syn20540628). Alternatively, I can use
the **`synapser`** package to fetch the data via the Synapse API.

```{r}
synapser::synLogin()
cadsr_entity <- synapser::synGet("syn20540628", downloadLocation = data_dir)
cadsr_entity$path
```

```{r}
# Read the 'caDSR-export-20190528-1320.tsv' file, wherever it's stored
cadsr_df <- readr::read_tsv(cadsr_entity$path)
```


## Construct reference tables

I'll start by normalizing the caDSR export table into a two-column dataframe with data element (DE) identifiers (CDE IDs) mapped to all corresponding synonyms. Rather than trying to directly match column headers in the input table to standard data element names, I'll use synonyms to give a bit more flexibility and hopefully increase the number of hits.

Let's look at an example with a single DE (`2192199` for `"Race Category Text"`).

```{r}
syn_str <- cadsr_df %>% 
  dplyr::filter(CDE_ID == "2192199") %>% 
  dplyr::select(CDE_SYNONYMS_MACHINE) %>%
  purrr::flatten_chr()

syn_str
```

Based on the description of the **`CDE_SYNONYMS_MACHINE`** column on the [**Reference Standards** page](https://www.synapse.org/#!Synapse:syn18065892/wiki/599381), I can figure out what the various pieces of this string represent:

> Collection of CDE short name and *all* computer-friendly alternate names. The names in the collection are pipe-delimited.

Using this information, I can use a function I wrote (`expand_syns()`) to parse and expand the list of synonyms so there is a separate row for each.

```{r}
cadsr_df %>% 
  dplyr::filter(CDE_ID == "2192199") %>% 
  expand_syns()
```

I'll apply this expansion to the full caDSR table and save for later use.

```{r}
cde_syn_df <- expand_syns(cadsr_df)
```

I also want to normalize the mapping between caDSR DEs and their respective permissible values (PVs). Looking at the same example DE (`"Race Category Text"`), here's the string listing all permissible values from the caDSR export:

```{r}
pv_str <- cadsr_df %>% 
  dplyr::filter(CDE_ID == "2192199") %>% 
  dplyr::select(PERMISSIBLE_VALUES) %>%
  purrr::flatten_chr()

pv_str
```

Based on the description of the **`PERMISSIBLE_VALUES`** column on the [**Reference Standards** page](https://www.synapse.org/#!Synapse:syn18065892/wiki/599381), I can figure out what the various pieces of this string represent:

> Collection of **Permissible Values (PV)**; each PV entry has the PV value, the value-description, and the vocabulary concept identifiers (if they exist). Each PV entry in the collection is pipe-delimited; the PV sub-fields are back slash-delimited (i.e., value\\text-value\\concept-code)

Using this information, I can use a function I wrote (**`pv_to_table()`**) to parse that string into a structured table as follows

```{r}
pv_to_table(pv_str)
```

Applying this operation over all DEs in the caDSR export is fairly time consuming, so I'll 'cache' the results by saving to a file on disk. The function doing most of the work here is **`expand_pvs()`**, which iteratively applies the **`pv_to_table()`** transformation to each row before flattening results into a single, expanded dataframe.

```{r, message=FALSE, warning=FALSE}
cadsr_pv_expanded_file = fs::path_join(c(user_data_dir, 
                                         "cadsr_pv_expanded.feather"))
if (!fs::file_exists(cadsr_pv_expanded_file)) {
  pv_concept_df <- cadsr_df %>%
    dplyr::filter(VALUE_DOMAIN_TYPE == "Enumerated",
                  !is.na(PERMISSIBLE_VALUES)) %>% 
    dplyr::select(CDE_ID, PERMISSIBLE_VALUES) %>%
    expand_pvs()
  feather::write_feather(pv_concept_df, cadsr_pv_expanded_file)
} else {
  pv_concept_df <- feather::read_feather(cadsr_pv_expanded_file)
}

head(pv_concept_df)
```

The expanded dataframe includes the full set of information for each PV concept. For the sake of matching to the input data, I'll just keep a single attribute (`text_value`) mapped to the DE identifier. I'll also convert these values to lowercase (and trim whitespace) to help with matching. The **`str_trim_lower()`** function is one I wrote to combine steps for convenience.

```{r}
cde_pv_df <- pv_concept_df %>% 
  dplyr::select(CDE_ID, pv = text_value) %>% 
  dplyr::mutate(pv = str_trim_lower(pv)) %>% 
  dplyr::distinct()

head(cde_pv_df)
```

---

# Demo

## Load example data

Let's take a look at some real data. I'll start with one of the four **leaderboard datasets**, the **Apollo2** table. 

The **Apollo2** input table can be accessed and downloaded from Synapse [**here**](https://www.synapse.org/#!Synapse:syn21088742). Alternatively, I can use
the **`synapser`** package to fetch the data via the Synapse API.

**Note:** while the examples and explanations below use the **Apollo2** dataset, you can switch between other datasets by changing the value of `dataset_num`. The rest of the code *should* still work (even if the text doesn't match).

```{r}
dataset_ids <- list(
  "Apollo2" = "syn21088742",
  "Outcome-Predictors" = "syn21088743",
  "REMBRANDT" = "syn21088744",
  "ROI-Masks" = "syn21088745"
)

datasets <- names(dataset_ids)
dataset_num <- 1
dataset_name <- datasets[dataset_num]

dataset_entity <- synapser::synGet(dataset_ids[[dataset_name]], 
                                   downloadLocation = input_dir,
                                   ifcollision = "overwrite.local")
dataset_entity$path
```

I also have access to the manually ascribed annotations, stored in Synapse as structured JSON files. I can download the annotation for **Apollo2** [**here**](https://www.synapse.org/#!Synapse:syn21431292) or use **`synapser`**.

```{r}
anno_dataset_ids <- list(
  "Apollo2" = "syn21431292",
  "Outcome-Predictors" = "syn21431291",
  "REMBRANDT" = "syn21431290",
  "ROI-Masks" = "syn21431289"
)

anno_dataset_entity <- synapser::synGet(anno_dataset_ids[[dataset_name]], 
                                        downloadLocation = data_dir,
                                        ifcollision = "overwrite.local")
anno_dataset_entity$path
```

I can now read the input table into a dataframe (`input_df`) and the annotated JSON file into a list object (`anno_data`).

**Note:** if I was more thorough in designing my baseline algorithm, I could probably take advantage of the **`readr`** library's inferred types for each column (see the `Parsed with column specification:` message below).


```{r}
missing_anno_cols <- c("neoplasm_histologic_grade_1", 
                       "Neurological Exam Outcome")

# To make things a bit easier for switching between datasets, I'll use a simple
# template to take advantage of the 'glue' library's string literal features
path_template <- "{dir_name}{prefix}{dset_name}.{ext}"

input_df <- readr::read_tsv(
  glue::glue(path_template,
             dir_name = input_dir,
             prefix = "",
             dset_name = dataset_name,
             ext = "tsv")
) %>% 
  dplyr::select_at(dplyr::vars(-dplyr::one_of(missing_anno_cols)))

anno_data <- jsonlite::read_json(
  glue::glue(path_template,
             dir_name = data_dir,
             prefix = "Annotated-",
             dset_name = dataset_name,
             ext = "json")
)
```


## Inspecting the data

In order to demonstrate the annotation logic below, I'll focus on the first column in the input data. The primary target of our annotation efforts for this challenge is the overall *column* itself, as encapsulated by the column's **header value (HV)**.

```{r}
demo_col_num <- 1
demo_col_hv <- names(input_df)[demo_col_num]

demo_col_hv
```

Checking the manual annotation for this column, I can see the expected result to semantically describe what the column represents. The annotation includes structured information about the **data element (DE)** — sourced from caDSR — that curators have interpreted to define this particular column of (meta)data based on the header and individual row values.

```{r}
anno_col_data <- anno_data$columns[[demo_col_num]]
anno_res_data <- anno_col_data$results[[1]]
anno_res_hv <- anno_res_data$result

# I find the 'prettified' JSON a bit easier to view than the printed R list
anno_res_hv %>% 
  jsonlite::toJSON(auto_unbox = TRUE, pretty = TRUE)
```

So, I know that curators chose DE `2192199` or `"Race Category Text"` as the best match for the `"race"` column. I'll see if I can develop an algorithm to produce the same result automatically...

Next I'll check out the individual values in the *rows* of the first column. In contrast to the "permissible values" specified for a particular DE, the **observed values (OV)** represent the raw values we find in rows of the data.

```{r}
demo_col_ov <- get_col_ov(input_df, demo_col_num)

head(demo_col_ov)
```

Like the column headers, curators have also annotated row values according to standard vocabularies, based on the object and property concepts of the matched DE (**`OBJECT_CLASS_IDS`** and **`PROPERTY_IDS`** columns in the caDSR table, respectively).

```{r,}
anno_res_ov <- anno_res_data$observedValues

anno_res_ov %>% 
  head() %>% 
  jsonlite::toJSON(auto_unbox = TRUE, pretty = TRUE)
```


## Annotating the data

Getting to the interesting stuff... For the sake of demonstrating a baseline tool, I plan to *only* use the caDSR export content to identify and select matches. While all results must correspond to existing caDSR standards, participants can use whatever other sources or strategies they choose to discover the best match.

### Matching by similarity between header and DE synonyms

I'll start by searching for any DEs for which one or more synonyms *contain* the HV for the current column. This search (using the the HV as a regular expression) and any preprocessing (conversion to lowercase, trimming of whitespace) is performed by the **`match_hv_syn()`** function (i.e., match DEs based on similarity between input HV and caDSR synonyms).

```{r}
demo_col_hv_syn_hits <- match_hv_syn(demo_col_hv, cde_syn_df)

demo_col_hv_syn_hits
```

While it's great that I got some candidate hits, I'd obviously like to narrow the results down a bit from `r nrow(demo_col_hv_syn_hits)`.

### Matching by overlap between observed and permissible values

I'm making the fairly naive assumption that values in the rows of a column are "enumerated" (i.e., fall within a pre-defined range of options). I'll include some checks later on to guess whether the column might actually be some other datatype... but to keep things simple, I'll stick to a default next step of matching OVs to PVs.

The **`match_ov_pv()`** function performs a basic *inner join* to find the intersection between unique OVs in the column and expanded PVs in the dataframe I constructed above.

```{r}
demo_col_ov_pv_hits <- match_ov_pv(demo_col_ov, cde_pv_df)

demo_col_ov_pv_hits
```

I now have two different sets of match candidates of different sizes, with (presumably) some overlapping and some non-overlapping DEs.

### Combining matches

I'll use a simple inner join here to get the intersection between the HV- and OV-based match candidates collected above.

```{r}
demo_col_de_hits <- demo_col_hv_syn_hits %>% 
  dplyr::inner_join(demo_col_ov_pv_hits, by = "CDE_ID") %>% 
  dplyr::distinct(CDE_ID)

demo_col_de_hits
```

I still have a bunch of hits, so I'll need to find a way to rank these results and select the best candidates.

### Ranking and filtering matches

This step is a bit of a hack... I previously mentioned that my default assumption is that each column is enumerated. If I don't find any hits under this assumption, then I double check whether there were any matches only based on the HV compared to synonyms — in this case, I treat the column as *non-enumerated*. If neither of these conditions is satisfied, then I settle for *no match*.

```{r}
is_enum_de <- TRUE
if (nrow(demo_col_de_hits) == 0) {
  enum_hits <- cadsr_df %>% 
    filter(CDE_ID %in% demo_col_hv_syn_hits$CDE_ID,
           VALUE_DOMAIN_TYPE == "Enumerated")
  is_enum_de <- nrow(enum_hits) > 0
  demo_col_de_hits <- demo_col_hv_syn_hits
}
is_nonenum_de <- !is_enum_de & (nrow(demo_col_hv_syn_hits) > 0)

glue::glue("Enumerated? {is_enum_de}\nNon-enumerated? {is_nonenum_de}")
```

If the column appears to be enumerated, I'll try to expand/improve my results a bit by doing a "fuzzy" search* — capturing DEs for which PVs partially (but not exactly) match OVs. Based on these results, I'll select the top 3 results based on (a) which fraction of the columns OVs are covered and (b) the minimum average string distance between OVs and PVs. Any tiebreakers are broken at random.

\*The logic used by **`match_ov_pv()`** when `fuzzy = TRUE` is pretty messy, so I won't get into it here, but you can check out the code on GitHub [**here**](https://github.com/Sage-Bionetworks/metadata-automation-challenge/blob/master/R/baseline_annotator.R#L79-L110).

If I think the column is non-enumerated, I'll just pick the top 3 DE results at random.

```{r}
n_results <- 3

if (is_enum_de) {
  demo_col_de_results <- cde_pv_df %>% 
    dplyr::filter(CDE_ID %in% demo_col_de_hits$CDE_ID) %>% 
    match_ov_pv(demo_col_ov, ., fuzzy = TRUE, n_results)
} else if (is_nonenum_de) {
  set.seed(0)
  demo_col_de_results <- cadsr_df %>% 
    dplyr::filter(CDE_ID %in% demo_col_hv_syn_hits$CDE_ID,
                  !VALUE_DOMAIN_TYPE == "Enumerated") %>% 
    dplyr::select(CDE_ID) %>% 
    dplyr::mutate(coverage = NA, mean_dist = NA) %>% 
    dplyr::sample_n(n_results)
}

demo_col_de_results
```

Taking a look at my top results...

```{r}
cadsr_df %>% 
  dplyr::select(CDE_ID, CDE_LONG_NAME, DEFINITION) %>% 
  dplyr::left_join(demo_col_de_results, ., by = "CDE_ID") %>% 
  dplyr::select(-coverage, -mean_dist)
```

The manually annotated DE (`"Race Category Text"`) is my third best result... so it won't get me max points — but still not bad.

### Collecting and formatting results

The next few functions are designed to convert results in tabular (dataframe) format into the structured JSON required for submissions.

```{r}
de_id <- demo_col_de_results$CDE_ID[1]

collect_result_hv(cadsr_df, de_id) %>% 
    jsonlite::toJSON(auto_unbox = TRUE, pretty = TRUE)
```


```{r, message=FALSE, warning=FALSE}
collect_result_ov(cadsr_df, de_id, demo_col_ov, enum = is_enum_de) %>% 
  head() %>% 
  jsonlite::toJSON(auto_unbox = TRUE, pretty = TRUE)
```

The **`collect_result()`** function combines the **`collect_result_hv()`** and **`collect_result_ov()`** functions to aggregate and format results for a single result for the current column.

```{r, message=FALSE, warning=FALSE}
collect_result(1, de_id, demo_col_ov, cadsr_df, enum = is_enum_de) %>% 
  purrr::modify_depth(1, head) %>% 
  jsonlite::toJSON(auto_unbox = TRUE, pretty = TRUE)
```

---

Putting it all together...

# Annotating tables

The **`annotate_column()`** function combines all of the steps above to produce properly formatted results for a single column.

```{r, message=FALSE, warning=FALSE}
n_results <- 3
annotate_column(
  input_df, demo_col_num, n_results, cadsr_df, cde_syn_df, cde_pv_df
) %>% 
  purrr::modify_depth(2, head) %>% 
  jsonlite::toJSON(auto_unbox = TRUE, pretty = TRUE)
```

I can now apply the **`annotate_column()`** function across all columns in the table to generate the full results. I'll include a little message showing the input column number and HV to track progress.

```{r, warning=FALSE}
submission_data <- list(
  "columns" = purrr::imap(names(input_df), function(.x, .y) {
    cat(paste0(.y, ": ", .x, "\n"))
    list(
      "columnNumber" = .y,
      "headerValue" = .x,
      "results" = annotate_column(
        input_df, 
        .y, 
        n_results, 
        cadsr_df,
        cde_syn_df,
        cde_pv_df
      )
    )
  })
)
```

---

# Evaluating results

Finally, I can use the **`get_overall_score()`** function to compare the baseline tool's results to the manual annotations.

```{r}
get_overall_score(submission_data, anno_data)
```

---

# Session info

```{r}
sessionInfo()
```



